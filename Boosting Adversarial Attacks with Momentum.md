# Boosting Adversarial Attacks with Momentum（2018）

大多数对抗性攻击只能欺骗黑箱模型，成功率很低。

为了解决这个问题，我们提出了一种基于动量的迭代算法来增强对抗性攻击。

具体做法：

通过将动量项集成到攻击的迭代过程中，我们的方法可以稳定更新方向，并在迭代过程中摆脱较差的局部最大值，从而获得更具可转移性的对抗示例。为了进一步提高黑盒攻击的成功率，我们将动量迭代算法应用于模型集合，并表明具有较强防御能力的对抗性训练模型也容易受到我们的黑盒攻击。我们希望所提出的方法将作为评估各种深度模型和防御方法的鲁棒性的基准。通过这种方法，我们在NIPS 2017非目标对抗性攻击和目标对抗性攻击比赛中获得了第一名。

专业术语

adversarial examples

对抗性示例的一个严重问题是它的很好的迁移性。为一个模型做的对抗性示例对其他模型也是对抗的。

这一点使得黑盒攻击变得可行，造成了安全问题。



现有的攻击方法在攻击黑盒模型时成功率很低，尤其是带有防御机制的黑盒模型.

具体表现在：

由于攻击能力和可转移性之间的权衡，现有的大多数方法都无法以黑盒方式成功攻击它们。

基于优化和迭代方法生成的对抗样例具有较差的可转移性，使得黑盒攻击的有效性降低。

基于一步梯度的方法产生了更多可转移的对抗示例，但由于他们白盒模型的成功率通常较低，这使得对黑盒攻击无效。

与以前研究不同的点和我们的创新点表现在

在本文中，我们提出了一大类基于动量迭代梯度的方法来提高生成的对抗性示例的成功率。

方法1.除了基于迭代梯度的方法用梯度迭代扰动输入以使损失函数最大化之外

方法2.基于动量的方法在迭代过程中在损失函数的梯度方向积累速度矢量，以稳定更新方向，避免局部极值差。

我们证明了动量迭代方法生成的对抗性示例在白盒攻击和黑盒攻击中都具有更高的成功率。所提出的方法缓解了白盒攻击与可转移性之间的权衡，是一种比一步法[5]和普通迭代法[9]更强的攻击算法。



为了进一步提高对抗性示例的可转移性，我们研究了几种攻击模型集合的方法，因为如果一个对抗性示例欺骗了多个模型，那么它更有可能对其他黑箱模型保持对抗性[12]。

我们证明了由多个模型的动量迭代方法生成的对抗示例可以成功地欺骗由集成对抗训练[24]以黑箱方式获得的鲁棒模型。

本文的发现为开发更健壮的深度学习模型提出了新的安全问题，希望我们的攻击将被用作评估各种深度学习模型和防御方法的鲁棒性的基准。总之，我们做了以下几点

我们引入了一类基于动量迭代梯度的攻击算法，在这种算法中，我们在每次迭代中积累损失函数的梯度，以稳定优化并避免局部极值差。

•我们研究了几种同时攻击多个模型的集成方法，通过保持较高的攻击成功率，证明了强大的可转移能力。

•我们首次表明，通过集成对抗训练获得的具有强大防御能力的模型也容易受到黑盒攻击。



对抗性攻击的一般规则是$\Vert x^* - x\Vert_p\le\epsilon$​，p可以是0，1，2，∞

## 攻击方法

### One-step gradient-based approaches

FGSM属于一步梯度法，关于FGSM可以看这篇文章

[[I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In ICLR, 2015. 1, 2, 3]]()

[1]: https://arxiv.org/abs/1412.6572	"Explaining and harnessing adversarial examples"

FGSM是通过最大损失化函数$J(x^*,y)$,$J$​通常是交叉熵损失

​							$$x^*=x+\epsilon\cdot sign(\nabla_x J(x,y))$$

FGM是FGSM的推广

​							$$x^*=x+\epsilon\cdot  \frac{\nabla_x J(x,y)}{\Vert\nabla_x J(x,y)\Vert_2} $$

### Iterative methods

### Optimization-based methods

## 实验

实验设置

研究了七个模型

四个模型是正常训练的模型

Inception v3 (Inc-v3)、Inception v4 (Inc-v4)、Inception Resnet v2 (regc -v2)、Resnet v2152 (Res-152)

另外三个是通过集成对抗训练训练的模型——inc -v3ens3、Inc-v3ens4、regc -v2ens。我们将简单地将最后三个模型称为“对抗训练模型”。

攻击单个模型

攻击模型集合

## 总结

在本文中，我们提出了一大类基于动量的迭代方法来增强对抗性攻击，它可以有效地欺骗白盒模型和黑盒模型。我们的方法在黑盒方式上始终优于基于一步梯度的方法和普通迭代方法。我们进行了大量的实验来验证所提出方法的有效性，并解释为什么它们在实践中有效。为了进一步提高生成的对抗性示例的可转移性，我们建议攻击一个逻辑融合在一起的模型集合。我们表明，通过集成对抗训练获得的模型容易受到我们的黑盒攻击，这为开发更健壮的深度学习模型提出了新的安全问题。

